{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM Audio Generation - minimal version",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg0_8zp-utP2"
      },
      "source": [
        "# Generating Audio with LSTM \n",
        "\n",
        "This notebook will let you download a song from Youtube and model it with an **LSTM**\n",
        "\n",
        "This models the way each spectral frame follows another and can be used to generate new raw audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LWepcBCvDA0"
      },
      "source": [
        "## Install prerequisites and get code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St4eugxz1VGF"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # less warnings ...\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ_Ufe-HuJQP"
      },
      "source": [
        "!git clone https://github.com/ual-cci/music_gen_interaction_RTML.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW7r6xROudFy"
      },
      "source": [
        "# python libraries\n",
        "!pip install Pillow numpy opencv-python PyWavelets tqdm slugify\n",
        "!pip install -U Flask\n",
        "!pip install lws==1.2.6\n",
        "!pip install tflearn\n",
        "!pip install librosa==0.7.2\n",
        "!pip install numba==0.48\n",
        "!pip install mock"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAaR9elasbV7"
      },
      "source": [
        "%cd /content/music_gen_interaction_RTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p04Rnk5_uw19"
      },
      "source": [
        "## Download a sample audio:\n",
        "\n",
        "Note: replace the url with whatever music video you want - or upload a file directly ... You can use the ffmpeg to convert it to wav later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcANvMFAtb8e"
      },
      "source": [
        "# get a youtube downloader\n",
        "!sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl\n",
        "!sudo chmod a+rx /usr/local/bin/youtube-dl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryBJeuc0Fi72"
      },
      "source": [
        "# Set up training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDFUMf2bFidG"
      },
      "source": [
        "%cd /content/music_gen_interaction_RTML/\n",
        "\n",
        "from unittest.mock import Mock, MagicMock\n",
        "args = MagicMock(name='method')\n",
        "sample_rate = 22050\n",
        "\n",
        "# This will set the same setting for the training and the running of the model:\n",
        "args.lstm_layers = 3\n",
        "args.lstm_units = 128\n",
        "args.sample_rate = sample_rate\n",
        "args.sequence_length = 40\n",
        "args.async_loading = True\n",
        "args.amount_epochs = -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifpajCHOT-wq"
      },
      "source": [
        "# Pick a new song from Youtube \n",
        "\n",
        "## Insert your own url below and train \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ64KLqFumpx"
      },
      "source": [
        "youtube_url = \"https://www.youtube.com/watch?v=4cIWu5m8UmA\"\n",
        "\n",
        "!youtube-dl -ci -f \"bestaudio[ext=m4a]\" $youtube_url -o 'youtube_audio.m4a'\n",
        "!ffmpeg -i 'youtube_audio.m4a' -ac 2 -f wav full.wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9YozPg4JF2G"
      },
      "source": [
        "# Audio file preparation - this cuts a 1 minute sample from the audio file:\n",
        "!ffmpeg -y -ss 60 -i full.wav -t 60 -c copy sample.wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16zWYouyumpy"
      },
      "source": [
        "### You will probably need to train for 300 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnRHWucTumpy"
      },
      "source": [
        "number_of_epochs = 150 # will take cca 4min\n",
        "number_of_epochs = 300 # will take cca 8min\n",
        "\n",
        "song_name = \"sample.wav\" # < we will train the model on this wav file\n",
        "model_name = \"my_trained_model\" # < and then save the model under this name (check the .tfl files in the folder afterwards)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIIq_601umpy"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyZ3BNJefXzn"
      },
      "source": [
        "# ----[keep the same bellow]-------------------------------------------------------------------\n",
        "\n",
        "# takes time!\n",
        "!python training_handler.py -target_file $song_name -model_name $model_name -amount_epochs $number_of_epochs -batch_size 512 \\\n",
        "                            -lstm_layers $args.lstm_layers  -lstm_units $args.lstm_units -sample_rate $args.sample_rate -sequence_length $args.sequence_length\n",
        "from IPython.display import clear_output \n",
        "clear_output()\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "i = glob.glob(\"*.tfl.png\")\n",
        "from IPython.display import Audio, Image\n",
        "display(Image(i[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bGNWTO-UJSC"
      },
      "source": [
        "# Generate\n",
        "\n",
        "As Vit mentioned in the lecture, we start from a place in the original audio track, then use the model to keep predicting new audio frames. \n",
        "\n",
        "Unfortunately, sometimes it can get stuck in a loop, or just stop generating interesting things so we can give it a kick by jumping to a new point in the song. \n",
        "\n",
        "Once we've switched, we keep using the model to generate new audio frames!\n",
        "\n",
        "Below, you can specify a sequence of lengths and changes to the generation process to create a new audio output. \n",
        "\n",
        "The `sequence` array stores these points. Each item in is an array that stores the **start position** and **segment length**. The default has three, but you can add in as many as you want and/or change the existing values. \n",
        "\n",
        "```\n",
        "sequence = [\n",
        "  [start1, length1],\n",
        "  [start2, length2],\n",
        "  [start3, length3],\n",
        "  etc....\n",
        "]\n",
        "```\n",
        "\n",
        "\n",
        "The default code:\n",
        "\n",
        "* Starts generating 10% through the song and generates 200 frames \n",
        "\n",
        "* Moves to 60% through the song and generates 300 frames\n",
        "\n",
        "* Moves to 90% through the song and generates 150 frames\n",
        "\n",
        "**Try with your own!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa-30qYaGaWY"
      },
      "source": [
        "# Here you can change the paths to other trained models / other song files\n",
        "load_generative_seeds_from = song_name\n",
        "load_model_from = model_name+\".tfl\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv1Ylg7O1nrG"
      },
      "source": [
        "from server_handler import ServerHandler\n",
        "import settings\n",
        "\n",
        "my_settings = settings.Settings(args)\n",
        "print(\"Important Settings: settings.lstm_layers=\", my_settings.lstm_layers, \", settings.lstm_units=\", my_settings.lstm_units,\n",
        "              \", settings.sample_rate=\", my_settings.sample_rate)\n",
        "\n",
        "generation_handler = ServerHandler(my_settings, manual_loading = True)\n",
        "generation_handler.manual_init_song_model(load_generative_seeds_from, load_model_from)\n",
        "\n",
        "# slightly experimental interpolation through the latents while generating ...\n",
        "\n",
        "generation_handler.change_impulse(0.2) # set to 20% sharp\n",
        "\n",
        "sequence = [\n",
        "  #Starts generating 10% through the song and generates 200 frames \n",
        "  [0.1, 200],\n",
        "  #Moves to 60% through the song and generates 300 frames\n",
        "  [0.6, 300],\n",
        "  #Moves to 90% through the song and generates 150 frames\n",
        "  [0.9, 150]            \n",
        "]\n",
        "\n",
        "output_audio = []\n",
        "\n",
        "for i in sequence:\n",
        "  position_in_the_song = i[0]\n",
        "  requested_length = i[1]\n",
        "  generation_handler.change_impulse_smoothly_start(position_in_the_song) # allow interpolation\n",
        "  audio, predict, reconstruct = generation_handler.generate_audio_sample(requested_length, interactive_i=position_in_the_song)\n",
        "  output_audio.append(audio)\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H2iHKgMUmbD"
      },
      "source": [
        "# Play Audio\n",
        "\n",
        "If you want to download the audio, you can use the Colab File Explorer on the left <----. \n",
        "\n",
        "Find the file `generated_output_exp_concat.wav` and select **download**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKOA8aGg24Zh"
      },
      "source": [
        "import librosa\n",
        "from IPython.display import Audio, Image\n",
        "output_audio = np.concatenate(output_audio)\n",
        "\n",
        "out_name = 'generated_output_exp_concat.wav'\n",
        "librosa.output.write_wav(out_name, output_audio, sr=sample_rate)\n",
        "Audio(out_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieFSgNbIUoPL"
      },
      "source": [
        "# Clean up if you are going to train a new model from a different youtube video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gXtBaGR26oW"
      },
      "source": [
        "# fast cleanup\n",
        "!mkdir unused\n",
        "!mv *.wav unused/\n",
        "!mv *.m4a unused/"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}